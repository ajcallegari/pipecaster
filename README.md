# pipecaster
(in progress)

Pipecaster is a Python library for building multichannel machine learning pipelines and for in-pipeline screening of hyperparameters, models, data sources, and feature engineering steps.  The current version supports algorithms with the scikit-learn transformer and predictor interfaces.

## multichannel machine learning

When building an ML pipeline with inputs from multiple data sources or from multiple feature engineering methods, better predictive accuracy is sometimes obtained by keeping the different inputs siloed in separate channels through initial feature selection and machine learning steps and then generating ensemble predictions via predictor voting or model stacking.  

Pipecaster provides a *MultichannelPipeline* class to simplify the construction of multichannel ensemble architectures.  Slice notation makes it easy to create wide pipelines (many inputs) by broadcasting operations across multiple input channels. A keras-like layer-by-layer construction makes it easy to build deep pipelines with one or more voting or stacking layers.  

*MultichannelPipeline* has the familiar scikit-learn transformer and predictor interfaces but with a list of input matrices in place of a single matrix:  

scikit-learn:  
`pipeline.fit(X, y).predict(X)`  
`pipeline.fit(X, y).transform(X)`  

pipecaster MultichannelPipeline:  
`pipeline.fit(Xs, y).predict(Xs)`  
`pipeline.fit(Xs, y).transform(Xs).`  

## in-pipeline screening
A typical ML workflow involves screening input sources, feature engineering steps, ML algorithms, and model hyperparameters.  Pipecaster allows you to semi-automate each of these screening tasks by including them in the ML pipeline.  This can be useful when you are developing a large number of different pipelines in parallel and don't have time to optimize each one separately, and it may accelerate ML workflows in general.  

In addition, pipecaster introduces channel selectors that select input channels based on aggregate feature scores or information content estimated using probe ML models or full ML models.  Channel selection is intended to prevent garbage from flowing into and out of your machine learning pipelines.

## fast distributed computing
Pipecaster uses the ray library to speed up multiprocessing by passing arguments through the plasma in-memory object store without the usual serialization/deserialization overhead and without passing the same object multiple times when needed in multiple jobs.  Ray also enables pipecaster to rapidly distribute jobs among networked computers.

# sample architecture
![Use case 1](/images/architecture_1.png)

This diagram shows a pipecaster classification pipeline taking 5 numerical input matrices (X0 to X4) and 1 text input (X5).  Code for building this pipeline is given below.  SelectKBestChannels computes a score for each input channel by aggregating their feature scores and then selects the k=3 best channels.  SelectKBestPredictors does an internal cross validation run within the training set during the call to pipeline.fit(Xs, y), estimates the accuracy of models trained on inputs 0 to 4, then selects the k=2 best models and sends their inferences on to a meta-classifier.

## sample code:

```
import pipecaster as pc  

clf = pc.Pipeline(n_inputs=6)

layer = clf.get_next_layer()
layer[:5] = SimpleImputer()
layer[5] = CountVectorizer()

layer = clf.get_next_layer()
layer[:5] = StandardScaler()
layer[5] = TfidfTransformer()

clf.get_next_layer()[:] = SelectKBest(f_classif, k = 100)

clf.get_next_layer()[:5] = pc.SelectKBestInputs(scoring=f_classif, aggregator='sum', k=3)

layer = clf.get_next_layer()
predictors = [KNeighborsClassifier() for i in range(5)]
layer[:5] = pc.SelectKBestPredictors(predictors=predictors,
                                     scoring=make_scorer(roc_auc_score), cv=3)
layer[5] = MultinomialNB()

clf.get_next_layer()[:] = pc.MetaClassifier(SVC())

clf.fit(X_trains, y_train)
clf.predict(X_tests)
```


# features

## meta-prediction with input channel ensembles
Inferences generated by siloed input channels can easily be combined through voting, averaging, and model stacking by layering pipecaster's *ConcatenatingPredictor* class onto your pipeline (see example above).  This pipeline architecture can be also be built in scikit-learn by nesting ColumnTransformers within a VotingClassifier/VotingRegressor or within a StackingClassifier/StackingRegressor.  Pipecaster automatically detects predictors with outputs used in meta-classification and provides them with internal cross validation training(1) and transform()/fit_transform() methods.  
(1) Wolpert, David H. "Stacked generalization." Neural networks 5.2 (1992): 241-259.

## in-pipeline screening

**Input screening**   
The different input channels passed to pipecaster pipelines (Xs) may come from different data sources, different transformations of the data (i.e. for feature engineering), or both.  Pipecaster provides three ways to select input channels in order to keep garbage from flowing into and out of your ML models.    

  1. The *ScoreChannelSelector* class selects input channels based on aggregated feature scores.  
  1. The *PerformanceChannelSelector* class selects input channels based on performance of a probe model on an internal cross validation run.
  1. The *ChannelModelSelector* is similar to the PerformanceChannelSelector, but outputs the predictions of selected models rather passing through the values from the previous pipeline step.  

**Model screening**  
Pipecaster allows in-pipeline screening of ML models and their hyperparameters with the *SelectiveEnsemble* class.  A *SelectiveEnsemble*, which operates on a single input, is a voting or concatenating ensemble that selects only the most performant models from within the ensemble. Model performance is assessed with an internal cross validation run within the training set during calls to pipeline.fit().  
