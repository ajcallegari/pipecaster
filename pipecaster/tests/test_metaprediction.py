import timeit
import multiprocessing
import ray
import numpy as np
import unittest
import warnings 

from scipy.stats import pearsonr

from sklearn.datasets import make_classification
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import roc_auc_score, accuracy_score

import sklearn.model_selection
import pipecaster.metaprediction
from pipecaster import synthetic_data
from pipecaster.pipeline import Pipeline
from pipecaster.metaprediction import MetaClassifier
from pipecaster.model_selection import cross_val_score


try:
    ray.nodes()
except RuntimeError:
    ray.init()

class TestSplitPredict(unittest.TestCase):
    
    def setUp(self):
        self.clf = RandomForestClassifier(n_estimators=20, random_state = 42)
        self.X, self.y = make_classification(n_samples=500, 
                                   n_features=400, 
                                   n_informative=200, 
                                   n_redundant=5,
                                   n_repeated=5,
                                   class_sep=1)
        
        self.sklearn_predictions = sklearn.model_selection.cross_val_predict(self.clf, self.X, self.y, cv = 5, n_jobs = 1)
        self.sklearn_auc = roc_auc_score(self.y, self.sklearn_predictions)

    def test_predictions(self):
        
        clf, X, y = self.clf, self.X, self.y

        pipecaster_predictions = pipecaster.metaprediction.split_predict(clf, X, y, cv = 5, n_jobs = 1)
        pipecaster_auc = roc_auc_score(y, self.sklearn_predictions)
        
        self.assertTrue(np.array_equal(self.sklearn_predictions, pipecaster_predictions),
                        'predictions from pipecaster.metaprediction.split_predict did not match sklearn version')

        self.assertTrue(self.sklearn_auc == pipecaster_auc,
                'auc of pipecaster.metaprediction.split_predict did not match sklearn version')
            
        self.assertTrue(pipecaster_auc > 0.5,
                'auc of pipecaster.metaprediction.split_predict was not better than random')
            
    def test_multiprocessing(self):
        clf, X, y = self.clf, self.X, self.y
        n_cpus = multiprocessing.cpu_count()

        if n_cpus > 1:
            # shut off warnings because ray and redis generate massive numbers
            warnings.filterwarnings("ignore")
            
            SETUP_CODE = ''' 
import pipecaster.model_selection'''
            TEST_CODE = ''' 
pipecaster.metaprediction.split_predict(clf, X, y, cv = 5, n_jobs = 1)'''
            t_serial = timeit.timeit(setup = SETUP_CODE, 
                                  stmt = TEST_CODE, 
                                  globals = locals(), 
                                  number = 5) 
            TEST_CODE = ''' 
pipecaster.metaprediction.split_predict(clf, X, y, cv = 5, n_jobs = {})'''.format(n_cpus)
            t_parallel = timeit.timeit(setup = SETUP_CODE, 
                                  stmt = TEST_CODE, 
                                  globals = locals(), 
                                  number = 5) 
    
            if t_serial <= t_parallel:
                warnings.warn('parallel cross_val_predict not faster than serial, likely issue with ray multiprocessing')

            parallel_predictions = pipecaster.metaprediction.split_predict(clf, X, y, cv = 5, n_jobs = n_cpus)
            parallel_auc = roc_auc_score(y, parallel_predictions)
            warnings.resetwarnings()
            
            self.assertTrue(np.array_equal(self.sklearn_predictions, parallel_predictions), 
                            'the predictions generated by parallel run of pipecaster.metaprediction.split_predict \
                             did not match sklearn control')
                        
            self.assertTrue(self.sklearn_auc == parallel_auc, 
                            'the auc of predictions generated by parallel run of pipecaster.metaprediction.split_predict \
                             did not match sklearn control')

            self.assertTrue(parallel_auc > 0.5, 
                            'the predictions generated by parallel run of pipecaster.metaprediction.split_predict \
                             did not match sklearn control')

class TestMetaClassifier(unittest.TestCase):
    
    def test_singe_matrix_soft_voting(self):
        """Determine if KNN->MetaClassifier(soft voting) in a pipecaster pipeline gives identical predictions to sklearn KNN on training data
        """
        X, y = make_classification(n_samples=100, n_features=20, n_informative=10, class_sep=5, random_state=42)
        clf = KNeighborsClassifier(n_neighbors=5, weights='uniform')
        clf.fit(X, y)
        clf_predictions = clf.predict(X)
        n_inputs = 1
        mclf = Pipeline(n_inputs)
        layer1 = mclf.get_next_layer()
        layer1[:] = clf
        layer2 = mclf.get_next_layer()
        layer2[:] = MetaClassifier('soft vote')
        mclf.fit([X], y)
        mclf_predictions = mclf.predict([X])
        self.assertTrue(np.array_equal(clf_predictions, mclf_predictions), 
                        'soft voting metaclassifier failed on single matrix easy prediction task')
        
    def test_singe_matrix_hard_voting(self):
        """Determine if KNN->MetaClassifier(hard voting) in a pipecaster pipeline gives identical predictions to sklearn KNN on training data
        """
        X, y = make_classification(n_samples=100, n_features=20, n_informative=10, class_sep=5, random_state=42)
        clf = KNeighborsClassifier(n_neighbors=5, weights='uniform')
        clf.fit(X, y)
        clf_predictions = clf.predict(X)
        n_inputs = 1
        mclf = Pipeline(n_inputs)
        layer1 = mclf.get_next_layer()
        layer1[:] = clf
        layer2 = mclf.get_next_layer()
        layer2[:] = MetaClassifier('hard vote')
        mclf.fit([X], y)
        mclf_predictions = mclf.predict([X])
        self.assertTrue(np.array_equal(clf_predictions, mclf_predictions), 
                        'hard voting metaclassifier failed on single matrix easy prediction task')
        
    def test_multi_matrix_voting(self):
        """Determine if KNN->MetaClassifier(soft voting) in a pipecaster pipeline gives monotonically increasing accuracy with increasing number of inputs in concordance with Condorcet's jury theorem, and also test hard voting with same pass criterion
        """
        
        n_jobs = multiprocessing.cpu_count()
        if n_jobs > 1:
            # shut off warnings because ray and redis generate massive numbers
            warnings.filterwarnings("ignore")
        
        n_inputs = 5
        soft_accuracies, hard_accuracies = [], []

        for i in range(0, n_inputs + 1):
            Xs, y, _ = synthetic_data.make_multi_input_classification(n_classes = 2, 
                                                        n_informative_Xs=i, 
                                                        n_weak_Xs=0,
                                                        n_random_Xs=n_inputs - i,   
                                                        n_samples=500, 
                                                        n_features=100, 
                                                        n_informative=30,
                                                        n_redundant=0,
                                                        n_repeated=0,
                                                        class_sep=3,
                                                        weak_noise_sd=None,
                                                        seed=None)
            mclf = Pipeline(n_inputs)
            layer0 = mclf.get_next_layer()
            layer0[:] = StandardScaler()
            layer1 = mclf.get_next_layer()
            layer1[:] = KNeighborsClassifier(n_neighbors=5, weights='uniform')
            layer2 = mclf.get_next_layer()
            layer2[:] = MetaClassifier('soft vote')

            split_accuracies = cross_val_score(mclf, Xs, y, prediction_method='predict', 
                                     scoring_metric=roc_auc_score, cv=3, n_jobs=1, verbose=0, 
                                     fit_params=None, error_score=np.nan)
            soft_accuracies.append(np.mean(split_accuracies))
            
            layer2.clear()
            layer2[:] = MetaClassifier('hard vote')
            split_accuracies = cross_val_score(mclf, Xs, y, prediction_method='predict', 
                                     scoring_metric=roc_auc_score, cv=3, n_jobs=1, verbose=0, 
                                     fit_params=None, error_score=np.nan)
            hard_accuracies.append(np.mean(split_accuracies))
            
        if n_jobs > 1:
            # shut off warnings because ray and redis generate massive numbers
            warnings.resetwarnings()
            
        n_informative = range(0, n_inputs + 1)
        self.assertTrue(soft_accuracies[-1] > 0.80)
        self.assertTrue(pearsonr(soft_accuracies, n_informative)[0] > 0.80)  
        self.assertTrue(hard_accuracies[-1] > 0.80)
        self.assertTrue(pearsonr(hard_accuracies, n_informative)[0] > 0.80) 
        
    def test_multi_matrices_svm_metaclassifier(self):
        
        n_jobs = multiprocessing.cpu_count()
        if n_jobs > 1:
            # shut off warnings because ray and redis generate massive numbers
            warnings.filterwarnings("ignore")

        n_inputs = 5
        accuracies = []

        for i in range(0, n_inputs + 1):
            Xs, y, _ = synthetic_data.make_multi_input_classification(n_classes = 2, 
                                                        n_informative_Xs=i, 
                                                        n_weak_Xs=0,
                                                        n_random_Xs=n_inputs - i,   
                                                        n_samples=500, 
                                                        n_features=100, 
                                                        n_informative=20,
                                                        n_redundant=0,
                                                        n_repeated=0,
                                                        class_sep=3,
                                                        weak_noise_sd=None,
                                                        seed=None)
            mclf = Pipeline(n_inputs)
            layer0 = mclf.get_next_layer()
            layer0[:] = StandardScaler()
            layer1 = mclf.get_next_layer()
            layer1[:] = KNeighborsClassifier(n_neighbors=5, weights='uniform')
            layer2 = mclf.get_next_layer()
            layer2[:] = MetaClassifier(SVC())

            split_accuracies = cross_val_score(mclf, Xs, y, prediction_method='predict', 
                                     scoring_metric=roc_auc_score, cv=3, n_jobs=n_jobs, verbose=0, 
                                     fit_params=None, error_score=np.nan)
            accuracies.append(np.mean(split_accuracies))
            
        if n_jobs > 1:
            # shut off warnings because ray and redis generate massive numbers
            warnings.resetwarnings()

        self.assertTrue(accuracies[-1] > 0.80)
        n_informative = range(0, n_inputs + 1)
        self.assertTrue(pearsonr(accuracies, n_informative)[0] > 0.80)

if __name__ == '__main__':
    unittest.main()