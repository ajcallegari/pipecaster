import random
import numpy as np
from sklearn.datasets import make_classification, make_regression

from pipecaster.utils import Cloneable

__all__ = ['make_multi_input_classification',
           'make_multi_input_classification', 'DummyClassifier']


def make_multi_input_classification(n_informative_Xs,
                                    n_weak_Xs=0,
                                    n_random_Xs=0,
                                    weak_noise_sd=0.2,
                                    seed=None, **sklearn_params):
    """
    Get a synthetic classification dataset with multiple input matrices of
    three possible types: informative, weak, and random.

    Parameters
    ---------
    n_informative_Xs: int,
        The number of informative feature matrices generated using the
        scikit-learn make_classification function.
    n_weak_Xs: int, default=0
        The number of feature matrices generated by adding a scikit-learn
        make_classification matrix to a Gaussian random deviate with a mean
        of zero and standard deviation set by the weak_noise_sd param.
    n_random_Xs: int, default=0
        The number of matrices genereated by shuffling the rows of a feature
        matrix generated by the scikit-learn make_classification function.
    sklearn_params: paramter dict
        Paramters for the sklearn make_classification function.
    weak_noise_sd: float, default=0.2
        Standard deviation of the Gaussian noise term used to
        generate weak matrices.
    seed: int or None, default=None
        Seed for pseudorandom number generators used in
        make_multi_input_classification.

    returns
    -------
    (Xs, y, X_types)
    Xs: list
        List of synthetic feature matrices in random order.  The rows in each
        matrix correspont to the samples in y.
    y: ndarray(dtype=int).shape(n_samples,)
        List of synthetic sample labels.
    X_types: list of strings,
        Description of the feature matrices: 'informative', 'weak', or 'random'
    """
    if seed is not None:
        random.seed(seed)
        np.random.seed(seed)
        sklearn_params['random_state'] = seed

    n_Xs = n_informative_Xs + n_weak_Xs + n_random_Xs
    n_samples = sklearn_params['n_samples']
    n_features = sklearn_params['n_features']

    for p in sklearn_params:
        if p in ['n_features', 'n_informative', 'n_redundant', 'n_repeated']:
            sklearn_params[p] *= n_Xs

    X, y = make_classification(**sklearn_params)

    # split synthetic data into separate matrices
    Xs = [X[:, i*n_features:(i+1)*n_features] for i in range(n_Xs)]

    # add extra gaussian noise to create weak matrices
    for i in range(n_weak_Xs):
        Xs[i] += np.random.normal(loc=0, scale=weak_noise_sd,
                                  size=(n_samples, n_features))

    # shuffle matrices that are neither informative or weak
    for i in range(n_weak_Xs, n_Xs - n_informative_Xs):
        np.random.shuffle(Xs[i])

    X_types = ['weak' for i in range(n_weak_Xs)]
    X_types += ['random' for i in range(n_random_Xs)]
    X_types += ['informative' for i in range(n_informative_Xs)]
    tuples = list(zip(Xs, X_types))
    random.shuffle(tuples)
    Xs, X_types = zip(*tuples)

    return list(Xs), y, list(X_types)
    if seed is not None:
        random.seed(seed)
        np.random.seed(seed)
        sklearn_params['random_state'] = seed

    def get_X(y):
        y = np.array(y)
        labels = np.unique(y)
        n_samples = {label: np.sum(y == label) for label in labels}
        chunk_size = 100

        X_aggregate, y_aggregate = None, None
        enough_labels = False
        while enough_labels is False:
            X_new, y_new = make_classification(**sklearn_params)
            if X_aggregate is None:
                X_aggregate = X_new
            else:
                X_aggregate = np.concatenate([X_aggregate, X_new], axis=0)

            if y_aggregate is None:
                y_aggregate = y_new
            else:
                y_aggregate = np.concatenate([y_aggregate, y_new])

            some_incompletes = False
            for label in labels:
                if np.sum(y_aggregate == label) < n_samples[label]:
                    some_incompletes = True
            if some_incompletes is False:
                enough_labels = True

        X = np.empty((len(y), X_new.shape[1]))
        for label in labels:
            label_mask = y == label
            label_X = X_aggregate[y_aggregate == label]
            X[label_mask] = label_X[:np.sum(label_mask)]
        return X

    X, y = make_classification(**sklearn_params)
    Xs = [get_X(y) for i in range(n_informative_Xs)]
    X_types = ['informative' for i in range(n_informative_Xs)]
    Xs += [get_X(y) + np.random.normal(loc=0, scale=weak_noise_sd,
                                       size=X.shape)
           for i in range(n_weak_Xs)]
    X_types += ['weak' for i in range(n_weak_Xs)]
    Xs += [np.random.permutation(get_X(y)) for i in range(n_random_Xs)]
    X_types += ['random' for i in range(n_random_Xs)]
    X_list = list(zip(Xs, X_types))
    random.shuffle(X_list)
    Xs, X_types = zip(*X_list)

    return list(Xs), y, list(X_types)


def make_multi_input_regression(n_informative_Xs,
                                n_weak_Xs=0,
                                n_random_Xs=0,
                                weak_noise_sd=0.2,
                                seed=None, **sklearn_params):
    """
    Get a synthetic regression dataset with multiple input matrices of
    three possible types: informative, weak, and random.

    Parameters
    ---------
    n_informative_Xs: int,
        The number of informative feature matrices generated using the
        scikit-learn make_regression function.
    n_weak_Xs: int, default=0
        The number of feature matrices generated by adding a scikit-learn
        make_regression matrix to a Gaussian random deviate with a mean
        of zero and standard deviation set by the weak_noise_sd param.
    n_random_Xs: int, default=0
        The number of matrices genereated by shuffling the rows of a feature
        matrix generated by the scikit-learn make_regression function.
    sklearn_params: paramter dict
        Paramters for the sklearn make_regression function.
    weak_noise_sd: float, default=0.2
        Standard deviation of the Gaussian noise term used to
        generate weak matrices.
    seed: int or None, default=None
        Seed for pseudorandom number generators used in
        make_multi_input_regression.

    returns
    -------
    (Xs, y, X_types)
    Xs: list
        List of synthetic feature matrices in random order.  The rows in each
        matrix correspont to the samples in y.
    y: ndarray(dtype=int).shape(n_samples,)
        List of synthetic sample labels.
    X_types: list of strings,
        Description of the feature matrices: 'informative', 'weak', or 'random'
    """
    if seed is not None:
        random.seed(seed)
        np.random.seed(seed)
        sklearn_params['random_state'] = seed

    n_Xs = n_informative_Xs + n_weak_Xs + n_random_Xs
    n_samples = sklearn_params['n_samples']
    n_features = sklearn_params['n_features']

    for p in sklearn_params:
        if p in ['n_features', 'n_informative', 'n_redundant', 'n_repeated']:
            sklearn_params[p] *= n_Xs

    X, y = make_regression(**sklearn_params)

    # split synthetic data into separate matrices
    Xs = [X[:, i*n_features:(i+1)*n_features] for i in range(n_Xs)]

    # add extra gaussian noise to create weak matrices
    for i in range(n_weak_Xs):
        Xs[i] += np.random.normal(loc=0, scale=weak_noise_sd,
                                  size=(n_samples, n_features))

    # shuffle matrices that are neither informative or weak
    for i in range(n_weak_Xs, n_Xs - n_informative_Xs):
        np.random.shuffle(Xs[i])

    X_types = ['weak' for i in range(n_weak_Xs)]
    X_types += ['random' for i in range(n_random_Xs)]
    X_types += ['informative' for i in range(n_informative_Xs)]
    tuples = list(zip(Xs, X_types))
    random.shuffle(tuples)
    Xs, X_types = zip(*tuples)

    return list(Xs), y, list(X_types)
    if seed is not None:
        random.seed(seed)
        np.random.seed(seed)
        sklearn_params['random_state'] = seed

    def get_X(y):
        y = np.array(y)
        labels = np.unique(y)
        n_samples = {label: np.sum(y == label) for label in labels}
        chunk_size = 100

        X_aggregate, y_aggregate = None, None
        enough_labels = False
        while enough_labels is False:
            X_new, y_new = make_regression(**sklearn_params)
            if X_aggregate is None:
                X_aggregate = X_new
            else:
                X_aggregate = np.concatenate([X_aggregate, X_new], axis=0)

            if y_aggregate is None:
                y_aggregate = y_new
            else:
                y_aggregate = np.concatenate([y_aggregate, y_new])

            some_incompletes = False
            for label in labels:
                if np.sum(y_aggregate == label) < n_samples[label]:
                    some_incompletes = True
            if some_incompletes is False:
                enough_labels = True

        X = np.empty((len(y), X_new.shape[1]))
        for label in labels:
            label_mask = y == label
            label_X = X_aggregate[y_aggregate == label]
            X[label_mask] = label_X[:np.sum(label_mask)]
        return X

    X, y = make_regression(**sklearn_params)
    Xs = [get_X(y) for i in range(n_informative_Xs)]
    X_types = ['informative' for i in range(n_informative_Xs)]
    Xs += [get_X(y) + np.random.normal(loc=0, scale=weak_noise_sd,
                                       size=X.shape)
           for i in range(n_weak_Xs)]
    X_types += ['weak' for i in range(n_weak_Xs)]
    Xs += [np.random.permutation(get_X(y)) for i in range(n_random_Xs)]
    X_types += ['random' for i in range(n_random_Xs)]
    X_list = list(zip(Xs, X_types))
    random.shuffle(X_list)
    Xs, X_types = zip(*X_list)

    return list(Xs), y, list(X_types)


class DummyClassifier(Cloneable):
    """
    Classifier that outputs random labels or probabilities and decouples
    argument passing overhead from fitting overhead or prediction overhead
    using futile cycles (e.g. it allows fast prediction with big input data or
    slow prediction with small input data, which is useful for distributed
    computing performance optimization).
    """
    def __init__(self, futile_cycles_fit=1000000, futile_cycles_pred=10):
        self.futile_cycles_fit = futile_cycles_fit
        self.futile_cycles_pred = futile_cycles_pred
        self._estimator_type = 'classifier'

    def fit(self, X, y=None, **fit_params):

        if y is not None:
            self.classes_, y_enc = np.unique(y, return_inverse=True)
        else:
            y_enc = y
        a = 0
        for i in range(self.futile_cycles_fit):
            a += 1
        return self

    def predict(self, X):
        a = 0
        for i in range(self.futile_cycles_pred):
            a += 1
        return np.random.choice(self.classes_, X.shape[0])

    def predict_proba(self, X):
        a = 0
        for i in range(self.futile_cycles_pred):
            a += 1
        return np.random.rand(X.shape[0], len(self.classes_))
