import random
import numpy as np
from sklearn.datasets import make_classification, make_regression

from pipecaster.utils import Cloneable

__all__ = ['make_multi_input_classification',
           'make_multi_input_regression', 'DummyClassifier']


def make_multi_input_classification(n_informative_Xs=5,
                                    n_weak_Xs=0,
                                    n_random_Xs=0,
                                    weak_noise_sd=0.2,
                                    seed=None, **sklearn_params):
    """
    Get a synthetic classification dataset with multiple input matrices of
    three possible types: informative, weak, and random.

    Parameters
    ---------
    n_informative_Xs: int,
        The number of informative feature matrices generated using the
        scikit-learn make_classification function.
    n_weak_Xs: int, default=0
        The number of feature matrices generated by adding a scikit-learn
        make_classification matrix to a Gaussian random deviate with a mean
        of zero and standard deviation set by the weak_noise_sd param.
    n_random_Xs: int, default=0
        The number of matrices genereated by shuffling the rows of a feature
        matrix generated by the scikit-learn make_classification function.
    sklearn_params: paramter dict
        Paramters for the sklearn make_classification function.
    weak_noise_sd: float, default=0.2
        Standard deviation of the Gaussian noise term used to
        generate weak matrices.
    seed: int or None, default=None
        Seed for pseudorandom number generators.  Int values enable
        reproduciblility.
    sklearn_params: keyword arguments or dict
        Parameters for scikit-learn's make_classification function which
        generates the dataset.  Parameters are per-matrix, not for entire
        multi-matrix dataset.

    returns
    -------
    (Xs, y, X_types)
    Xs: list
        List of synthetic feature matrices in random order.  The rows in each
        matrix correspont to the samples in y.
    y: ndarray(dtype=int).shape(n_samples,)
        List of synthetic sample labels.
    X_types: list of strings,
        Description of the feature matrices: 'informative', 'weak', or 'random'
    """
    if seed is not None:
        random.seed(seed)
        np.random.seed(seed)
        sklearn_params['random_state'] = seed

    def get_X(y):
        y = np.array(y)
        labels = np.unique(y)
        n_samples = {label: np.sum(y == label) for label in labels}
        chunk_size = 100

        X_aggregate, y_aggregate = None, None
        enough_labels = False
        while enough_labels is False:
            X_new, y_new = make_classification(**sklearn_params)
            if X_aggregate is None:
                X_aggregate = X_new
            else:
                X_aggregate = np.concatenate([X_aggregate, X_new], axis=0)

            if y_aggregate is None:
                y_aggregate = y_new
            else:
                y_aggregate = np.concatenate([y_aggregate, y_new])

            some_incompletes = False
            for label in labels:
                if np.sum(y_aggregate == label) < n_samples[label]:
                    some_incompletes = True

            if some_incompletes is False:
                enough_labels = True

            if seed is not None:
                sklearn_params['random_state'] += 1

        X = np.empty((len(y), X_new.shape[1]))
        for label in labels:
            label_mask = y == label
            label_X = X_aggregate[y_aggregate == label]
            X[label_mask] = label_X[:np.sum(label_mask)]
        return X

    X, y = make_classification(**sklearn_params)
    Xs = [get_X(y) for i in range(n_informative_Xs)]
    X_types = ['informative' for i in range(n_informative_Xs)]
    Xs += [get_X(y) + np.random.normal(loc=0, scale=weak_noise_sd,
                                       size=X.shape)
           for i in range(n_weak_Xs)]
    X_types += ['weak' for i in range(n_weak_Xs)]
    Xs += [np.random.permutation(get_X(y)) for i in range(n_random_Xs)]
    X_types += ['random' for i in range(n_random_Xs)]
    X_list = list(zip(Xs, X_types))
    random.shuffle(X_list)
    Xs, X_types = zip(*X_list)

    return list(Xs), y, list(X_types)


def make_multi_input_regression(n_informative_Xs=10,
                                n_weak_Xs=0,
                                n_random_Xs=0,
                                weak_noise_sd=0.2,
                                seed=None, **sklearn_params):
    """
    Get a synthetic regression dataset with multiple input matrices of
    three possible types: informative, weak, and random.

    Parameters
    ---------
    n_informative_Xs: int,
        The number of informative feature matrices generated using the
        scikit-learn make_regression function.
    n_weak_Xs: int, default=0
        The number of feature matrices generated by adding a scikit-learn
        make_regression matrix to a Gaussian random deviate with a mean
        of zero and standard deviation set by the weak_noise_sd param.
    n_random_Xs: int, default=0
        The number of matrices genereated by shuffling the rows of a feature
        matrix generated by the scikit-learn make_regression function.
    sklearn_params: paramter dict
        Paramters for the sklearn make_regression function.
    weak_noise_sd: float, default=0.2
        Standard deviation of the Gaussian noise term used to
        generate weak matrices.
    seed: int or None, default=None
        Seed for pseudorandom number generators.  Int values enable
        reproduciblility.
    sklearn_params: keyword arguments or dict
        Parameters for scikit-learn's make_classification function which
        generates the dataset.  Parameters are per-matrix, not for entire
        multi-matrix dataset.

    returns
    -------
    (Xs, y, X_types)
    Xs: list
        List of synthetic feature matrices in random order.  The rows in each
        matrix correspont to the samples in y.
    y: ndarray(dtype=int).shape(n_samples,)
        List of synthetic sample labels.
    X_types: list of strings,
        Description of the feature matrices: 'informative', 'weak', or 'random'
    """

    if seed is not None:
        random.seed(seed)
        np.random.seed(seed)
        sklearn_params['random_state'] = seed

    X_types = ['informative' for i in range(n_informative_Xs)]
    X_types += ['weak' for i in range(n_weak_Xs)]
    X_types += ['random' for i in range(n_random_Xs)]

    n_Xs = n_informative_Xs + n_weak_Xs + n_random_Xs

    n_inf = sklearn_params['n_informative']
    n_rand = sklearn_params['n_features'] - sklearn_params['n_informative']

    sklearn_params['n_features'] *= n_Xs
    sklearn_params['n_informative'] *= n_Xs
    sklearn_params['coef'] = True

    if 'effective_rank' in sklearn_params:
        if sklearn_params['effective_rank'] is not None:
            print('Warning: When effective rank is set, information not \
              guaranteed to be equally distributed between feature matrices')

    X_pool, y, coef = make_regression(**sklearn_params)
    informative_mask = coef != 0
    X_pool_inf = X_pool[:, informative_mask]
    X_pool_rand = X_pool[:, ~informative_mask]

    Xs = []
    for i, X_type in enumerate(X_types):
        X_inf = X_pool_inf[:, i*n_inf:(i+1)*n_inf]
        X_rand = X_pool_rand[:, i*n_rand:(i+1)*n_rand]
        X = np.concatenate([X_inf, X_rand], axis=1)
        X = X[:, np.random.permutation(X.shape[1])]
        if X_type == 'weak':
            X += np.random.normal(loc=0, scale=weak_noise_sd, size=X.shape)
        elif X_type == 'random':
            np.random.shuffle(X)
        Xs.append(X)

    tuples = list(zip(Xs, X_types))
    random.shuffle(tuples)
    Xs, X_types = zip(*tuples)

    return list(Xs), y, list(X_types)


class DummyClassifier(Cloneable):
    """
    Classifier that outputs random labels or probabilities and decouples
    argument passing overhead from fitting overhead or prediction overhead
    using futile cycles (e.g. it allows fast prediction with big input data or
    slow prediction with small input data, which is useful for distributed
    computing performance optimization).
    """
    def __init__(self, futile_cycles_fit=1000000, futile_cycles_pred=10):
        self.futile_cycles_fit = futile_cycles_fit
        self.futile_cycles_pred = futile_cycles_pred
        self._estimator_type = 'classifier'

    def fit(self, X, y=None, **fit_params):

        if y is not None:
            self.classes_, y_enc = np.unique(y, return_inverse=True)
        else:
            y_enc = y
        a = 0
        for i in range(self.futile_cycles_fit):
            a += 1
        return self

    def predict(self, X):
        a = 0
        for i in range(self.futile_cycles_pred):
            a += 1
        return np.random.choice(self.classes_, X.shape[0])

    def predict_proba(self, X):
        a = 0
        for i in range(self.futile_cycles_pred):
            a += 1
        return np.random.rand(X.shape[0], len(self.classes_))
